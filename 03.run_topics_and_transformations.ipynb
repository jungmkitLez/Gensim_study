{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T11:36:28.137259Z",
     "start_time": "2021-09-06T11:36:27.519794Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Topics and Transformations\n",
    "===========================\n",
    "\n",
    "토이 corpus에서 변환과정을 실습해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T11:36:28.152776Z",
     "start_time": "2021-09-06T11:36:28.138264Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 튜토리얼 에서는 한 document를 한 벡터에서 다른 벡터로 represent 하는 법을 배웁니다.\n",
    "이번 과정은 두가지 목표를 가지고 있습니다.\n",
    "\n",
    "1. corpus의 내재된 구조를 꺼내고, 단어들 사이의 관계를 탐색합니다.그리고 그것들은 document를 새롭고 의미론 적으로 표현해봅니다.\n",
    "\n",
    "2. document representation을 좀더 치밀하게 하려면, \n",
    "    효율(적은 리소스로 새로운 represatation 생성)과 효능(중요하지 않은 데이터를 무시)을 증가시켜야 합니다.\n",
    "\n",
    "Creating the Corpus\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T11:53:54.610540Z",
     "start_time": "2021-09-06T11:53:54.588543Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-06 20:53:54,590 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-09-06 20:53:54,591 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "2021-09-06 20:53:54,592 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\", 'datetime': '2021-09-06T20:53:54.592542', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x25916513190>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]\n",
    "\n",
    "# 불용어 제거\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "# 한번 나온 단어제거\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T11:53:59.814453Z",
     "start_time": "2021-09-06T11:53:59.799454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (5, 2), (8, 1)],\n",
       " [(3, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(4, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformation 생성 <br>\n",
    "+++++++++++++++++++++<br><br>\n",
    "transformation은 training corpus들로 만들어진 python 객체들 입니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T11:54:21.685936Z",
     "start_time": "2021-09-06T11:54:21.673936Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-06 20:54:21,674 : INFO : collecting document frequencies\n",
      "2021-09-06 20:54:21,674 : INFO : PROGRESS: processing document #0\n",
      "2021-09-06 20:54:21,675 : INFO : TfidfModel lifecycle event {'msg': 'calculated IDF weights for 9 documents and 12 features (28 matrix non-zeros)', 'datetime': '2021-09-06T20:54:21.675938', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'initialize'}\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)  # step 1 -- 모델 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 transformation은 다른 초기화 파라미터들이 필요할 수 있습니다.<br>\n",
    "tfidt의 경우에는 단어의 문장안의 빈도수와 전체 corpus에서의 빈도수를 이용합니다.<br>\n",
    "<br>\n",
    "Latent Semantic Analysis 또는 Latent Dirichlet Allocation은 더 진보된 방식이지만 결국 더 많은 시간이 필요합니다.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>\n",
    "  Transformations는 항상 두 특정 벡터공간 사이의 변환입니다.\n",
    "  모델 훈련에는 동일한 벡터공간이 사용되어져야 합니다.\n",
    "  다음에 올 벡터 transformation들도 마찬가지 입니다.\n",
    "  문자열처리 방식이 다르다 거나, id가 다르게 동일한 특성공간을 사용하지 못하면,\n",
    "  transformation이 호출 되었을때 feature mismatch가 발생하거나 나쁜 출력이 나오거나\n",
    "  런타임오류가 발생할 수 있습니다.\n",
    "</p></div>\n",
    "\n",
    "\n",
    "벡터 변환\n",
    "+++++++++++++++++++++\n",
    "\n",
    "지금부터 tfidf는 어떤 벡터를 representation으로 변환하는 읽기전용 객체로 다뤄집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T11:36:30.220479Z",
     "start_time": "2021-09-06T11:36:30.206211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow])  # step 2 -- 벡터변환을 하기위해 모델을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 corpus에 transformation 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T11:36:30.236092Z",
     "start_time": "2021-09-06T11:36:30.221480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 경우에, 모델학습에 사용된 corpus를 벡터 변환 했습니다.<br>\n",
    "그러나 모델이 한번 초기화 된 이후에도 어떤 단어가 corpus training에 전혀 사용되지 않았다해도<br>\n",
    "그 벡터를 사용 할 수 있습니다.<br>\n",
    "\n",
    "이는 folding-in 방식의 LSA와 topic inference 방식의 LDA가 할 수 있습니다.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4>\n",
    "    <p>corpus_transformed = model[corpus] 처럼 corpus를 변환 하는 것은 모든 결과를 메모리에 적재한다는 것입니다.<br>\n",
    "    이는 gensim의 메모리와 독립적이려는 의도와 반하는 일입니다.<br>\n",
    "        그래서 모델의 결과를 corpus format으로 먼저 직렬화하고 그것을 iterate해서 사용하는 것이 좋습니다.\n",
    "  .</p></div>\n",
    "\n",
    "Transformations 또한 직렬화 될 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T11:36:30.296154Z",
     "start_time": "2021-09-06T11:36:30.237094Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-06 20:36:30,238 : INFO : using serial LSI version on this node\n",
      "2021-09-06 20:36:30,238 : INFO : updating model with new documents\n",
      "2021-09-06 20:36:30,239 : INFO : preparing a new chunk of documents\n",
      "2021-09-06 20:36:30,240 : INFO : using 100 extra samples and 2 power iterations\n",
      "2021-09-06 20:36:30,240 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2021-09-06 20:36:30,246 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2021-09-06 20:36:30,265 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2021-09-06 20:36:30,275 : INFO : computing the final decomposition\n",
      "2021-09-06 20:36:30,275 : INFO : keeping 2 factors (discarding 47.565% of energy spectrum)\n",
      "2021-09-06 20:36:30,276 : INFO : processed documents up to #9\n",
      "2021-09-06 20:36:30,278 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2021-09-06 20:36:30,279 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n",
      "2021-09-06 20:36:30,279 : INFO : LsiModel lifecycle event {'msg': 'trained LsiModel(num_terms=12, num_topics=2, decay=1.0, chunksize=20000) in 0.04s', 'datetime': '2021-09-06T20:36:30.279121', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)  # initialize an LSI transformation\n",
    "corpus_lsi = lsi_model[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Latent Semantic Indexing <http://en.wikipedia.org/wiki/Latent_semantic_indexing>`<br><br>\n",
    "\n",
    "TF-idf corpus를 2차원 (num_topic을 2로 줬기 때문에)Latent Semantic Indexing 로 변환을 시켰습니다.<br>\n",
    "그럼 궁금한 것이 이 두 잠재차원은 무엇을 위해 존재하는 걸까요?<br>\n",
    "models.LsiModel.print_topics 함수로 알아 봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T11:36:30.311467Z",
     "start_time": "2021-09-06T11:36:30.297156Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-06 20:36:30,298 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2021-09-06 20:36:30,299 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  '-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T11:47:25.701056Z",
     "start_time": "2021-09-06T11:47:25.694055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.06600783396090475), (1, -0.5200703306361845)] Human machine interface for lab abc computer applications\n",
      "[(0, 0.19667592859142663), (1, -0.760956316770004)] A survey of user opinion of computer system response time\n",
      "[(0, 0.08992639972446669), (1, -0.7241860626752508)] The EPS user interface management system\n",
      "[(0, 0.07585847652178374), (1, -0.6320551586003426)] System and human system engineering testing of EPS\n",
      "[(0, 0.10150299184980256), (1, -0.5737308483002953)] Relation of user perceived response time to error measurement\n",
      "[(0, 0.7032108939378304), (1, 0.16115180214025948)] The generation of random binary unordered trees\n",
      "[(0, 0.8774787673119824), (1, 0.16758906864659617)] The intersection graph of paths in trees\n",
      "[(0, 0.9098624686818572), (1, 0.14086553628719245)] Graph minors IV Widths of trees and well quasi ordering\n",
      "[(0, 0.6165825350569281), (1, -0.05392907566389192)] Graph minors A survey\n"
     ]
    }
   ],
   "source": [
    "# bow->tfidf 로 바꾸고 tfidf->lsi 로 변환하는 과정은 실제로 여기서 일어남\n",
    "for doc, as_text in zip(corpus_lsi, documents):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save load 기능을 모델에 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T12:15:11.722345Z",
     "start_time": "2021-09-06T12:15:11.695032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-06 21:15:11,697 : INFO : Projection lifecycle event {'fname_or_handle': 'C:\\\\Users\\\\yongjae\\\\AppData\\\\Local\\\\Temp\\\\model-df0gwmn9.lsi.projection', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-09-06T21:15:11.697031', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}\n",
      "2021-09-06 21:15:11,698 : INFO : saved C:\\Users\\yongjae\\AppData\\Local\\Temp\\model-df0gwmn9.lsi.projection\n",
      "2021-09-06 21:15:11,699 : INFO : LsiModel lifecycle event {'fname_or_handle': 'C:\\\\Users\\\\yongjae\\\\AppData\\\\Local\\\\Temp\\\\model-df0gwmn9.lsi', 'separately': 'None', 'sep_limit': 10485760, 'ignore': ['projection', 'dispatcher'], 'datetime': '2021-09-06T21:15:11.699032', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}\n",
      "2021-09-06 21:15:11,699 : INFO : not storing attribute projection\n",
      "2021-09-06 21:15:11,700 : INFO : not storing attribute dispatcher\n",
      "2021-09-06 21:15:11,701 : INFO : saved C:\\Users\\yongjae\\AppData\\Local\\Temp\\model-df0gwmn9.lsi\n",
      "2021-09-06 21:15:11,702 : INFO : loading LsiModel object from C:\\Users\\yongjae\\AppData\\Local\\Temp\\model-df0gwmn9.lsi\n",
      "2021-09-06 21:15:11,702 : INFO : loading id2word recursively from C:\\Users\\yongjae\\AppData\\Local\\Temp\\model-df0gwmn9.lsi.id2word.* with mmap=None\n",
      "2021-09-06 21:15:11,703 : INFO : setting ignored attribute projection to None\n",
      "2021-09-06 21:15:11,703 : INFO : setting ignored attribute dispatcher to None\n",
      "2021-09-06 21:15:11,704 : INFO : LsiModel lifecycle event {'fname': 'C:\\\\Users\\\\yongjae\\\\AppData\\\\Local\\\\Temp\\\\model-df0gwmn9.lsi', 'datetime': '2021-09-06T21:15:11.704032', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'loaded'}\n",
      "2021-09-06 21:15:11,704 : INFO : loading LsiModel object from C:\\Users\\yongjae\\AppData\\Local\\Temp\\model-df0gwmn9.lsi.projection\n",
      "2021-09-06 21:15:11,705 : INFO : Projection lifecycle event {'fname': 'C:\\\\Users\\\\yongjae\\\\AppData\\\\Local\\\\Temp\\\\model-df0gwmn9.lsi.projection', 'datetime': '2021-09-06T21:15:11.705032', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='model-', suffix='.lsi', delete=False) as tmp:\n",
    "    lsi_model.save(tmp.name)  # same for tfidf, lda, ...\n",
    "\n",
    "loaded_lsi_model = models.LsiModel.load(tmp.name)\n",
    "\n",
    "os.unlink(tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 질문은 아마도 정확히 document들이 서로 얼마나 비슷하냐 일것이다.\n",
    "유사도를 정형화 하는 방법이 있기 때문에, 우리는 입력 document에 대해서 \n",
    "유사도 순으로 document들을 정렬 할수있을까요? Similarity queries는 다음 튜토리얼에서.\n",
    "\n",
    "Available transformations\n",
    "--------------------------\n",
    "\n",
    "가능한 변환 방법들 (tf-idf, LSI, RP, LDA, HDP, VSM)\n",
    "\n",
    "Gensim implements several popular Vector Space Model algorithms:\n",
    "\n",
    "* `Term Frequency * Inverse Document Frequency, Tf-Idf <http://en.wikipedia.org/wiki/Tf%E2%80%93idf>`_\n",
    "  expects a bag-of-words (integer values) training corpus during initialization.\n",
    "  During transformation, it will take a vector and return another vector of the\n",
    "  same dimensionality, except that features which were rare in the training corpus\n",
    "  will have their value increased.\n",
    "  It therefore converts integer-valued vectors into real-valued ones, while leaving\n",
    "  the number of dimensions intact. It can also optionally normalize the resulting\n",
    "  vectors to (Euclidean) unit length.\n",
    "\n",
    " .. sourcecode:: pycon\n",
    "\n",
    "    model = models.TfidfModel(corpus, normalize=True)\n",
    "\n",
    "* `Latent Semantic Indexing, LSI (or sometimes LSA) <http://en.wikipedia.org/wiki/Latent_semantic_indexing>`_\n",
    "  transforms documents from either bag-of-words or (preferrably) TfIdf-weighted space into\n",
    "  a latent space of a lower dimensionality. For the toy corpus above we used only\n",
    "  2 latent dimensions, but on real corpora, target dimensionality of 200--500 is recommended\n",
    "  as a \"golden standard\" [1]_.\n",
    "\n",
    "  .. sourcecode:: pycon\n",
    "\n",
    "    model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=300)\n",
    "\n",
    "  LSI training is unique in that we can continue \"training\" at any point, simply\n",
    "  by providing more training documents. This is done by incremental updates to\n",
    "  the underlying model, in a process called `online training`. Because of this feature, the\n",
    "  input document stream may even be infinite -- just keep feeding LSI new documents\n",
    "  as they arrive, while using the computed transformation model as read-only in the meanwhile!\n",
    "\n",
    "  .. sourcecode:: pycon\n",
    "\n",
    "    model.add_documents(another_tfidf_corpus)  # now LSI has been trained on tfidf_corpus + another_tfidf_corpus\n",
    "    lsi_vec = model[tfidf_vec]  # convert some new document into the LSI space, without affecting the model\n",
    "\n",
    "    model.add_documents(more_documents)  # tfidf_corpus + another_tfidf_corpus + more_documents\n",
    "    lsi_vec = model[tfidf_vec]\n",
    "\n",
    "  See the :mod:`gensim.models.lsimodel` documentation for details on how to make\n",
    "  LSI gradually \"forget\" old observations in infinite streams. If you want to get dirty,\n",
    "  there are also parameters you can tweak that affect speed vs. memory footprint vs. numerical\n",
    "  precision of the LSI algorithm.\n",
    "\n",
    "  `gensim` uses a novel online incremental streamed distributed training algorithm (quite a mouthful!),\n",
    "  which I published in [5]_. `gensim` also executes a stochastic multi-pass algorithm\n",
    "  from Halko et al. [4]_ internally, to accelerate in-core part\n",
    "  of the computations.\n",
    "  See also `wiki` for further speed-ups by distributing the computation across\n",
    "  a cluster of computers.\n",
    "\n",
    "* `Random Projections, RP <http://www.cis.hut.fi/ella/publications/randproj_kdd.pdf>`_ aim to\n",
    "  reduce vector space dimensionality. This is a very efficient (both memory- and\n",
    "  CPU-friendly) approach to approximating TfIdf distances between documents, by throwing in a little randomness.\n",
    "  Recommended target dimensionality is again in the hundreds/thousands, depending on your dataset.\n",
    "\n",
    "  .. sourcecode:: pycon\n",
    "\n",
    "    model = models.RpModel(tfidf_corpus, num_topics=500)\n",
    "\n",
    "* `Latent Dirichlet Allocation, LDA <http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>`_\n",
    "  is yet another transformation from bag-of-words counts into a topic space of lower\n",
    "  dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA),\n",
    "  so LDA's topics can be interpreted as probability distributions over words. These distributions are,\n",
    "  just like with LSA, inferred automatically from a training corpus. Documents\n",
    "  are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).\n",
    "\n",
    "  .. sourcecode:: pycon\n",
    "\n",
    "    model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)\n",
    "\n",
    "  `gensim` uses a fast implementation of online LDA parameter estimation based on [2]_,\n",
    "  modified to run in `distributed mode <distributed>` on a cluster of computers.\n",
    "\n",
    "* `Hierarchical Dirichlet Process, HDP <http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf>`_\n",
    "  is a non-parametric bayesian method (note the missing number of requested topics):\n",
    "\n",
    "  .. sourcecode:: pycon\n",
    "\n",
    "    model = models.HdpModel(corpus, id2word=dictionary)\n",
    "\n",
    "  `gensim` uses a fast, online implementation based on [3]_.\n",
    "  The HDP model is a new addition to `gensim`, and still rough around its academic edges -- use with care.\n",
    "\n",
    "Adding new :abbr:`VSM (Vector Space Model)` transformations (such as different weighting schemes) is rather trivial;\n",
    "see the `apiref` or directly the `Python code <https://github.com/piskvorky/gensim/blob/develop/gensim/models/tfidfmodel.py>`_\n",
    "for more info and examples.\n",
    "\n",
    "It is worth repeating that these are all unique, **incremental** implementations,\n",
    "which do not require the whole training corpus to be present in main memory all at once.\n",
    "With memory taken care of, I am now improving `distributed`,\n",
    "to improve CPU efficiency, too.\n",
    "If you feel you could contribute by testing, providing use-cases or code, see the `Gensim Developer guide <https://github.com/RaRe-Technologies/gensim/wiki/Developer-page>`__.\n",
    "\n",
    "What Next?\n",
    "----------\n",
    "\n",
    "Continue on to the next tutorial on `sphx_glr_auto_examples_core_run_similarity_queries.py`.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    ".. [1] Bradford. 2008. An empirical study of required dimensionality for large-scale latent semantic indexing applications.\n",
    "\n",
    ".. [2] Hoffman, Blei, Bach. 2010. Online learning for Latent Dirichlet Allocation.\n",
    "\n",
    ".. [3] Wang, Paisley, Blei. 2011. Online variational inference for the hierarchical Dirichlet process.\n",
    "\n",
    ".. [4] Halko, Martinsson, Tropp. 2009. Finding structure with randomness.\n",
    "\n",
    ".. [5] Řehůřek. 2011. Subspace tracking for Latent Semantic Analysis.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
